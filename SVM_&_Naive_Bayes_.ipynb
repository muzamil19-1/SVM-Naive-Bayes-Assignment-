{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:  What is a Support Vector Machine (SVM), and how does it work?**\n",
        "\n",
        " Answer:  A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks, but it is most widely applied to classification problems.\n",
        "\n",
        "How it Works:\n",
        "\n",
        "Decision Boundary (Hyperplane):\n",
        "\n",
        "SVM tries to find the best separating boundary (called a hyperplane) that divides the data points of different classes.\n",
        "\n",
        "In 2D, this boundary is a straight line; in 3D, it’s a plane; in higher dimensions, it’s called a hyperplane.\n",
        "\n",
        "Maximizing the Margin:\n",
        "\n",
        "Among all possible hyperplanes, SVM chooses the one that maximizes the margin — the distance between the hyperplane and the nearest data points of each class.\n",
        "\n",
        "These closest data points are called support vectors (they “support” or define the boundary).\n",
        "\n",
        "Handling Non-Linear Data:\n",
        "\n",
        "When the data is not linearly separable, SVM uses the kernel trick to map data into a higher-dimensional space where it becomes separable.\n",
        "\n",
        "Common kernels:\n",
        "\n",
        "Linear kernel\n",
        "\n",
        "Polynomial kernel\n",
        "\n",
        "Radial Basis Function (RBF) kernel\n",
        "\n",
        "Soft Margin & Regularization:\n",
        "\n",
        "Real-world data may contain noise or overlapping classes.\n",
        "\n",
        "SVM allows some misclassifications by introducing a soft margin controlled by a parameter C:\n",
        "\n",
        "Large C: tries to classify everything correctly (less tolerance for errors).\n",
        "\n",
        "Small C: allows more misclassifications for a wider margin (better generalization).\n",
        "\n",
        "**Question 2: Explain the difference between Hard Margin and Soft Margin SVM.**\n",
        "\n",
        "Answer: In Support Vector Machines (SVM), the concepts of Hard Margin and Soft Margin refer to how strictly the algorithm separates the classes.\n",
        "\n",
        "1. Hard Margin SVM\n",
        "\n",
        "Definition: A hard margin SVM tries to find a hyperplane that perfectly separates the data without allowing any misclassification.\n",
        "\n",
        "Assumption: The data must be linearly separable (no overlap, no noise).\n",
        "\n",
        "Characteristics:\n",
        "\n",
        "No tolerance for errors.\n",
        "\n",
        "Maximizes the margin with strict separation.\n",
        "\n",
        "Works well only with clean, noise-free datasets.\n",
        "\n",
        "Prone to overfitting if the data has outliers or overlap.\n",
        "\n",
        "2. Soft Margin SVM\n",
        "\n",
        "Definition: A soft margin SVM allows some data points to be misclassified in order to create a wider margin and achieve better generalization.\n",
        "\n",
        "Controlled by Parameter C:\n",
        "\n",
        "Large C: Focuses on minimizing misclassification (narrow margin).\n",
        "\n",
        "Small C: Allows more misclassifications but improves margin (better generalization).\n",
        "\n",
        "Characteristics:\n",
        "\n",
        "Tolerant of noise and overlapping classes.\n",
        "\n",
        "Balances margin size with classification accuracy.\n",
        "\n",
        "More practical for real-world datasets.\n",
        "\n",
        "| Feature           | Hard Margin SVM                | Soft Margin SVM                           |\n",
        "| ----------------- | ------------------------------ | ----------------------------------------- |\n",
        "| Misclassification | Not allowed                    | Allowed (with penalty)                    |\n",
        "| Data Requirement  | Linearly separable, no noise   | Works with noisy/overlapping data         |\n",
        "| Flexibility       | Very rigid (strict separation) | Flexible, controlled by `C`               |\n",
        "| Risk              | Overfitting with noisy data    | Better generalization in real-world cases |\n",
        "\n",
        "\n",
        "**Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case. **\n",
        "\n",
        "\n",
        "Answer:  Kernel Trick in SVM\n",
        "\n",
        "The Kernel Trick is a mathematical technique in Support Vector Machines (SVM) that allows the algorithm to handle non-linear data by mapping it into a higher-dimensional feature space, where it becomes linearly separable.\n",
        "\n",
        "Instead of explicitly computing the transformation (which may be computationally expensive), the kernel trick uses a kernel function to calculate the similarity (dot product) between two data points in the higher-dimensional space without actually performing the transformation.\n",
        "\n",
        "How it Works (Simple Idea):\n",
        "\n",
        "Suppose data is not separable in 2D.\n",
        "\n",
        "By applying a kernel, we can imagine projecting it into 3D (or higher) where a linear hyperplane can separate the classes.\n",
        "\n",
        "The kernel trick makes this projection implicit and efficient.\n",
        "\n",
        "Example Kernel: Radial Basis Function (RBF) Kernel\n",
        "\n",
        "Formula:\n",
        "\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        ",\n",
        "𝑥\n",
        "′\n",
        ")\n",
        "=\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "−\n",
        "𝛾\n",
        "∥\n",
        "𝑥\n",
        "−\n",
        "𝑥\n",
        "′\n",
        "∥\n",
        "2\n",
        ")\n",
        "\n",
        "Explanation:\n",
        "\n",
        "Measures similarity between two points based on their distance.\n",
        "\n",
        "Nearby points → High similarity (close to 1).\n",
        "\n",
        "Faraway points → Low similarity (close to 0).\n",
        "\n",
        "Use Case:\n",
        "\n",
        "Works well for problems where the decision boundary is highly non-linear, such as:\n",
        "\n",
        "Image classification\n",
        "\n",
        "Handwriting recognition\n",
        "\n",
        "Bioinformatics (e.g., classifying protein sequences)\n",
        "\n",
        "✅ In short:\n",
        "\n",
        "The Kernel Trick allows SVM to classify non-linear data efficiently by computing similarities in higher dimensions.\n",
        "\n",
        "Example: RBF kernel → useful when classes are not linearly separable, like in image or text classification.\n",
        "\n",
        "\n",
        "**Question 4: What is a Naïve Bayes Classifier, and why is it called “naïve”? **\n",
        "\n",
        "Answer: A Naïve Bayes Classifier is a probabilistic machine learning algorithm that applies Bayes’ Theorem for classification tasks. It is commonly used in text classification, spam detection, sentiment analysis, and medical diagnosis.\n",
        "\n",
        "Bayes’ Theorem:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝐶\n",
        "∣\n",
        "𝑋\n",
        ")\n",
        "=\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        "∣\n",
        "𝐶\n",
        ")\n",
        "⋅\n",
        "𝑃\n",
        "(\n",
        "𝐶\n",
        ")/\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝐶\n",
        "∣\n",
        "𝑋\n",
        ")\n",
        "P(C∣X): Probability of class\n",
        "𝐶\n",
        "C given the features\n",
        "𝑋\n",
        "X (posterior).\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        "∣\n",
        "𝐶\n",
        ")\n",
        "P(X∣C): Probability of features given the class (likelihood).\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝐶\n",
        ")\n",
        "P(C): Prior probability of the class.\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        ")\n",
        "P(X): Probability of the features.\n",
        "\n",
        "Why is it called “Naïve”?\n",
        "\n",
        "It is called naïve because it makes a strong simplifying assumption:\n",
        "\n",
        "All features are independent of each other, given the class label.\n",
        "\n",
        "Example: In text classification, it assumes the occurrence of the word “money” is independent of the word “profit”, which is rarely true in reality.\n",
        "\n",
        "Despite this “naïve” assumption, the algorithm performs surprisingly well in practice, especially with large, high-dimensional datasets.\n",
        "\n",
        "Key Points:\n",
        "\n",
        "Advantages: Fast, simple, works well with text data, effective with small training sets.\n",
        "\n",
        "Limitations: Independence assumption may not always hold.\n",
        "\n",
        "Applications: Email spam filtering, document categorization, medical diagnosis, sentiment analysis.\n",
        "\n",
        "\n",
        "**Question 5: Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants. When would you use each one? **\n",
        "\n",
        "Answer: Naïve Bayes has several variants, each designed for different types of data. The three most common are Gaussian, Multinomial, and Bernoulli Naïve Bayes.\n",
        "\n",
        "1. Gaussian Naïve Bayes\n",
        "\n",
        "Assumption: The features follow a normal (Gaussian) distribution.\n",
        "\n",
        "Formula for likelihood:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        "∣\n",
        "𝐶\n",
        ")\n",
        "=\n",
        "1\n",
        "2\n",
        "𝜋\n",
        "𝜎\n",
        "2\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "−\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        "−\n",
        "𝜇\n",
        ")\n",
        "2\n",
        "2\n",
        "𝜎\n",
        "2\n",
        ")\n",
        "P(x\n",
        "i\n",
        "\t​\n",
        "\n",
        "∣C)=\n",
        "2πσ\n",
        "2\n",
        "\t​\n",
        "\n",
        "1\n",
        "\t​\n",
        "\n",
        "exp(−\n",
        "2σ\n",
        "2\n",
        "(x\n",
        "i\n",
        "\t​\n",
        "\n",
        "−μ)\n",
        "2\n",
        "\t​\n",
        "\n",
        ")\n",
        "\n",
        "Use Case:\n",
        "\n",
        "Suitable for continuous data (real-valued features).\n",
        "\n",
        "Examples: medical data (blood pressure, height, weight), sensor readings.\n",
        "\n",
        "2. Multinomial Naïve Bayes\n",
        "\n",
        "Assumption: Features represent discrete counts or frequencies.\n",
        "\n",
        "Intuition: The likelihood is based on how often a feature (like a word) occurs in a document.\n",
        "\n",
        "Use Case:\n",
        "\n",
        "Works well for text classification problems where features are word counts or term frequency.\n",
        "\n",
        "Examples: spam filtering, document classification, topic categorization.\n",
        "\n",
        "3. Bernoulli Naïve Bayes\n",
        "\n",
        "Assumption: Features are binary (0 or 1) — presence or absence of a feature.\n",
        "\n",
        "Intuition: Instead of how many times a word appears, it only matters whether it appears or not.\n",
        "\n",
        "Use Case:\n",
        "\n",
        "Suitable for datasets with binary/boolean features.\n",
        "\n",
        "Examples: text classification using word presence/absence, sentiment analysis (positive vs. negative word present).\n",
        "\n",
        "| Variant         | Data Type Assumption        | Typical Use Case                                                               |\n",
        "| --------------- | --------------------------- | ------------------------------------------------------------------------------ |\n",
        "| **Gaussian**    | Continuous (real values)    | Medical data, sensor data, continuous features                                 |\n",
        "| **Multinomial** | Discrete counts (frequency) | Text classification with word counts (spam detection, document classification) |\n",
        "| **Bernoulli**   | Binary (0/1)                | Text classification with word presence/absence, sentiment analysis             |\n",
        "\n",
        "\n",
        "**Question 6: Write a Python program to: ● Load the Iris dataset ● Train an SVM Classifier with a linear kernel ● Print the model's accuracy and support vectors. (Include your Python code and output in the code box below.) **\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "jchfi2VW-cYQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeeNr083-SJc",
        "outputId": "6e1e5472-6ac6-430c-e54f-3c1e1a957f64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Support Vectors:\n",
            " [[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data   # Features\n",
        "y = iris.target # Labels\n",
        "\n",
        "# 2. Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train an SVM Classifier with a linear kernel\n",
        "model = SVC(kernel='linear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 5. Print the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# 6. Print the support vectors\n",
        "print(\"Support Vectors:\\n\", model.support_vectors_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7:  Write a Python program to: ● Load the Breast Cancer dataset ● Train a Gaussian Naïve Bayes model ● Print its classification report including precision, recall, and F1-score. (Include your Python code and output in the code box below.)**\n",
        "\n",
        " Answer:  "
      ],
      "metadata": {
        "id": "b8M85ohZBUWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# 1. Load the Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X = cancer.data   # Features\n",
        "y = cancer.target # Labels\n",
        "\n",
        "# 2. Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train a Gaussian Naïve Bayes model\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 5. Print classification report\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=cancer.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G62hD2_BBJsQ",
        "outputId": "82c8e836-d08d-49bb-e22c-7483454f0868"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.93      0.90      0.92        63\n",
            "      benign       0.95      0.96      0.95       108\n",
            "\n",
            "    accuracy                           0.94       171\n",
            "   macro avg       0.94      0.93      0.94       171\n",
            "weighted avg       0.94      0.94      0.94       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: Write a Python program to: ● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best C and gamma. ● Print the best hyperparameters and accuracy. (Include your Python code and output in the code box below.)**\n",
        "\n",
        " Answer:  "
      ],
      "metadata": {
        "id": "Iyf9vP9mBiPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data   # Features\n",
        "y = wine.target # Labels\n",
        "\n",
        "# 2. Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Define parameter grid for C and gamma\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [0.001, 0.01, 0.1, 1],\n",
        "    'kernel': ['rbf']  # Using RBF kernel\n",
        "}\n",
        "\n",
        "# 4. Train SVM Classifier with GridSearchCV\n",
        "grid = GridSearchCV(SVC(), param_grid, refit=True, cv=5, verbose=0)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# 5. Predict on the test set\n",
        "y_pred = grid.predict(X_test)\n",
        "\n",
        "# 6. Print the best hyperparameters and accuracy\n",
        "print(\"Best Hyperparameters:\", grid.best_params_)\n",
        "print(\"Model Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-x4mvffBgNA",
        "outputId": "2836e747-0feb-474b-c2e7-9a0bfe5414eb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Model Accuracy: 0.7777777777777778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Write a Python program to: ● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using sklearn.datasets.fetch_20newsgroups). ● Print the model's ROC-AUC score for its predictions. (Include your Python code and output in the code box below.)**\n",
        "\n",
        " Answer:  "
      ],
      "metadata": {
        "id": "cSGg8YX6Bxs7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# 1. Load a subset of the 20 Newsgroups dataset (for speed, select few categories)\n",
        "categories = ['alt.atheism', 'sci.space', 'talk.politics.misc']\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories)\n",
        "\n",
        "X = newsgroups.data   # Text data\n",
        "y = newsgroups.target # Labels\n",
        "\n",
        "# 2. Convert text data to TF-IDF features\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "# 3. Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_tfidf, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 4. Train a Multinomial Naïve Bayes Classifier\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 5. Predict probabilities on the test set\n",
        "y_proba = model.predict_proba(X_test)\n",
        "\n",
        "# 6. Compute ROC-AUC score (multi-class, using 'ovr')\n",
        "y_test_bin = label_binarize(y_test, classes=[0, 1, 2])\n",
        "roc_auc = roc_auc_score(y_test_bin, y_proba, multi_class='ovr')\n",
        "\n",
        "# 7. Print the ROC-AUC score\n",
        "print(\"ROC-AUC Score:\", roc_auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6WW8NFBBt4a",
        "outputId": "dcc314d9-633c-4acb-c59e-42a75434fb6f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9990086115884346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: Imagine you’re working as a data scientist for a company that handles email communications. Your task is to automatically classify emails as Spam or Not Spam. The emails may contain: ● Text with diverse vocabulary ● Potential class imbalance (far more legitimate emails than spam) ● Some incomplete or missing data Explain the approach you would take to: ● Preprocess the data (e.g. text vectorization, handling missing data) ● Choose and justify an appropriate model (SVM vs. Naïve Bayes) ● Address class imbalance ● Evaluate the performance of your solution with suitable metrics And explain the business impact of your solution. (Include your Python code and output in the code box below.) **\n",
        "\n",
        "Answer:  Approach\n",
        "1. Data Preprocessing\n",
        "\n",
        "Handling Missing Data:\n",
        "\n",
        "Fill missing email text with an empty string (\"\").\n",
        "\n",
        "Drop rows if critical labels are missing.\n",
        "\n",
        "Text Vectorization:\n",
        "\n",
        "Use TF-IDF (Term Frequency–Inverse Document Frequency) to convert text into numerical features.\n",
        "\n",
        "Helps handle diverse vocabulary and reduces the impact of very common words.\n",
        "\n",
        "2. Model Choice (SVM vs. Naïve Bayes)\n",
        "\n",
        "Naïve Bayes works well for text data, is fast, and handles high-dimensional sparse features (like word counts) efficiently.\n",
        "\n",
        "SVM provides strong decision boundaries but is slower on large text datasets.\n",
        "\n",
        "✅ Chosen Model: Multinomial Naïve Bayes → efficient, interpretable, and widely used in spam filtering.\n",
        "\n",
        "3. Handling Class Imbalance\n",
        "\n",
        "Spam datasets often have far more legitimate emails than spam.\n",
        "\n",
        "Approaches:\n",
        "\n",
        "Use class weights to penalize misclassification of minority class.\n",
        "\n",
        "Apply resampling techniques (SMOTE or undersampling).\n",
        "\n",
        "Focus on precision & recall rather than just accuracy.\n",
        "\n",
        "4. Model Evaluation\n",
        "\n",
        "Use Confusion Matrix, Precision, Recall, F1-score, and ROC-AUC.\n",
        "\n",
        "Recall (Sensitivity) is important → missing spam emails (false negatives) is costly.\n",
        "\n",
        "Precision is also important → false positives may annoy users.\n",
        "\n",
        "5. Business Impact\n",
        "\n",
        "Reduces the number of spam emails reaching users.\n",
        "\n",
        "Saves employee time and prevents security risks (phishing, malware).\n",
        "\n",
        "Improves trust in the company’s email communication system.\n",
        "\n",
        "Python Code Implementation"
      ],
      "metadata": {
        "id": "m_RKPv8ECITX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Example synthetic dataset (replace with real email dataset)\n",
        "data = {\n",
        "    \"text\": [\n",
        "        \"Congratulations, you won a free lottery ticket\",\n",
        "        \"Please find attached the project report\",\n",
        "        \"Limited offer, buy now and save 50%\",\n",
        "        \"Let's schedule the meeting for tomorrow\",\n",
        "        \"Earn money quickly from home\",\n",
        "        \"Your invoice is attached\"\n",
        "    ],\n",
        "    \"label\": [\"spam\", \"ham\", \"spam\", \"ham\", \"spam\", \"ham\"]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 1. Handle missing values\n",
        "df['text'] = df['text'].fillna(\"\")\n",
        "\n",
        "# 2. Vectorize text using TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(df['text'])\n",
        "y = df['label'].map({\"ham\": 0, \"spam\": 1})  # Encode labels\n",
        "\n",
        "# 3. Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 4. Train Naïve Bayes model\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 5. Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_proba = model.predict_proba(X_test)[:,1]\n",
        "\n",
        "# 6. Evaluation\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=[\"ham\", \"spam\"]))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_proba))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71dvRKpdB90y",
        "outputId": "edb936e8-b95a-42e0-a672-9080fb4b1d81"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.50      1.00      0.67         1\n",
            "        spam       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.50         2\n",
            "   macro avg       0.25      0.50      0.33         2\n",
            "weighted avg       0.25      0.50      0.33         2\n",
            "\n",
            "Confusion Matrix:\n",
            " [[1 0]\n",
            " [1 0]]\n",
            "ROC-AUC Score: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_p9bVYLvCakI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}